[23:30:40] *** Joins: cmr (moznet@moz-EC676DFE.members.linode.com)
[23:34:31] <acrichto> strcat: also https://github.com/mozilla/rust/pull/9479 should help the raytracer benchmark
[23:35:26] <strcat> acrichto: aren't they still marked inline(never)?
[23:35:30] <cmr> I think it'd be worth adding the raytracer benchmark to src/test/bench
[23:35:33] <strcat> we can't inline fixed_stack_segment stuff in the stdlib yet
[23:37:14] <acrichto> strcat: no, fixed_stack_segment doesn't imply inline(never)
[23:37:19] <strcat> acrichto: I know
[23:37:24] <acrichto> cmr: it would if we ran the benchmarks :(
[23:37:26] <strcat> but you can't inline fixed_stack_segment stuff from the stdlib yet
[23:37:40] <strcat> you *can*, it will work
[23:37:45] <strcat> but it won't be correct
[23:37:46] <acrichto> some intrinsics are doing it right now...
[23:37:53] <acrichto> what's the reason that you can't inline again?
[23:37:59] <Eridius> strcat: I thought it would be correct, it would just apply fixed_stack_segment to the caller, which is typically a bad idea
[23:38:12] *** Joins: bjz (brendanzab@B398BE78.F804C29E.D35A31DF.IP)
[23:38:13] <Eridius> that's certainly what I was told last time I asked about this
[23:38:24] <acrichto> yeah brson was concerned that people who don't ask for fixed_stack_segment would get fixed_stack_segments which is bad
[23:38:30] <strcat> acrichto: causes memory to leak if the function yields
[23:38:51] <acrichto> yields?
[23:38:57] <strcat> deschedules
[23:38:58] <Eridius> strcat: well, only until the task resumes executing
[23:39:23] <strcat> which means segmented stacks aren't doing anything
[23:39:32] <strcat> except adding overhead
[23:39:48] <strcat> acrichto: there is a proposed fix for it
[23:39:56] *** Joins: gavin (gavin@moz-DB4A9C19.scl3.mozilla.com)
[23:40:03] *** gavin sets mode: +o brson
[23:40:21] <acrichto> what is it?
[23:40:40] <strcat> acrichto: teaching LLVM's inliner about the problem, and only inlining fixed-stack-segment fns into fixed-stack-segment fns
[23:40:58] <strcat> we could remove inline(never) from just these, but there are a *lot* more cases of this in the stdlib
[23:41:07] <strcat> exchange_malloc + exchange_free for example
[23:41:24] <acrichto> it's just one more "make your code fast" attribute to always remember  :(
[23:42:15] <brson> gavin: thanks!
[23:42:20] <Eridius> theoretically could the scheduler copy a fixed_stack_segment's stack back into a segmented stack section when descheduling, and then copy it back into a new fixed stack when rescheduling? Or would that be far too expensive?
[23:42:54] <Eridius> although that may not be doable in the case of a C function calling a rust callback
[23:43:07] <Eridius> and actually yeah, that's not doable in general because of addresses changing. Forget I said anything
[23:43:26] <strcat> acrichto: we can make it work at a module-level
[23:43:32] <strcat> and tag all functions as fixed_stack_segment
[23:43:42] <strcat> runtimeless rust is really inconvenient atm due to this ;p
[23:43:46] *** ChanServ sets mode: +tr 
[23:43:46] *** ChanServ sets mode: +q brson
[23:44:14] <strcat> huh, I thought +q was mute
[23:44:31] *** ChanServ sets mode: +q brson
[23:45:53] <Luqman> q on a user is founder/owner mode
[23:46:18] <dbaupp> acrichto: does https://github.com/mozilla/rust/pull/9479 actually make things faster?
[23:46:36] <acrichto> sadly not from what I measured
[23:46:41] <dbaupp> :(
